{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from snowflake.snowpark.session import Session\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "connection_params = {\n",
    "  \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "  \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "  \"password\": \"Passwordforsnowflake1\",\n",
    "  \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "  \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "  \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
    "  \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
    "}\n",
    "\n",
    "snowpark_session = Session.builder.configs(connection_params).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.session.Session at 0x2036bad34f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Snowflakes get their unique patterns through a complex process that involves both physics and chemistry. It all starts with a tiny particle in the atmosphere, like a dust or pollen grain, which serves as a nucleus for the snowflake to form around.\n",
      "\n",
      "As this particle cools, water vapor in the air begins to condense and freeze onto it, forming an ice crystal. The shape of this initial crystal is determined by the temperature and humidity conditions in the atmosphere at that time.\n",
      "\n",
      "As the ice crystal falls through the sky, it passes through different layers of air with varying temperatures and humidity levels. Each layer causes the ice crystal to grow and change in a unique way. The six-sided structure of snowflakes is due to the molecular structure of water, which forms hexagonal shapes as it freezes.\n",
      "\n",
      "The intricate patterns and branches of a snowflake are a result of the way water molecules arrange themselves as they freeze onto the existing crystal structure. The specific pattern depends on the temperature and humidity conditions at the time. For example, at around -15 degrees Celsius, snowflakes tend to form simple, flat plates or columns. At around -5 degrees Celsius, they tend to form more complex, branched structures.\n",
      "\n",
      "So, even though all snowflakes are made from the same basic material (water), the varying conditions in the atmosphere as they form and fall create an almost infinite variety of unique patterns.\n"
     ]
    }
   ],
   "source": [
    "from snowflake.cortex import Complete\n",
    "\n",
    "print(Complete(\"mistral-large\", \"how do snowflakes get their unique patterns?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "import os\n",
    "import re\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "client = GithubClient(github_token=github_token, verbose=False)\n",
    "\n",
    "reader = GithubRepositoryReader(\n",
    "    github_client=client,\n",
    "    owner=\"streamlit\",\n",
    "    repo=\"docs\",\n",
    "    use_parser=False,\n",
    "    verbose=True,\n",
    "    filter_directories=(\n",
    "        [\"content\"],\n",
    "        GithubRepositoryReader.FilterType.INCLUDE,\n",
    "    ),\n",
    "    filter_file_extensions=(\n",
    "        [\".md\"],\n",
    "        GithubRepositoryReader.FilterType.INCLUDE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "documents = reader.load_data(branch=\"main\")\n",
    "\n",
    "\n",
    "def clean_up_text(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove unwanted characters and patterns in text input.\n",
    "\n",
    "    :param content: Text input.\n",
    "\n",
    "    :return: Cleaned version of original text input.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fix hyphenated words broken by newline\n",
    "    content = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", content)\n",
    "\n",
    "    unwanted_patterns = [\"---\\nvisible: false\", \"---\", \"#\", \"slug:\"]\n",
    "    for pattern in unwanted_patterns:\n",
    "        content = re.sub(pattern, \"\", content)\n",
    "\n",
    "    # Remove all slugs starting with a \\ and stopping at the first space\n",
    "    content = re.sub(r\"\\\\slug: [^\\s]*\", \"\", content)\n",
    "\n",
    "    # normalize whitespace\n",
    "    content = re.sub(r\"\\s+\", \" \", content)\n",
    "    return content\n",
    "\n",
    "\n",
    "cleaned_documents = []\n",
    "\n",
    "for d in documents:\n",
    "    cleaned_text = clean_up_text(d.text)\n",
    "    # Create a new instance of the document with the cleaned text\n",
    "    cleaned_document = d.copy(update={\"text\": cleaned_text})\n",
    "    cleaned_documents.append(cleaned_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\Desktop\\ragnroll_test\\getting_started_llmops\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\"Snowflake/snowflake-arctic-embed-m\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=85, embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/333 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  3.80it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  6.86it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.34it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:03<00:00,  4.91it/s]\n",
      "Generating embeddings: 100%|██████████| 42/42 [00:09<00:00,  4.54it/s]\n",
      "Generating embeddings: 100%|██████████| 48/48 [00:08<00:00,  5.63it/s]\n",
      "Generating embeddings: 100%|██████████| 27/27 [00:05<00:00,  4.82it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:03<00:00,  4.50it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:04<00:00,  4.02it/s]\n",
      "Generating embeddings: 100%|██████████| 53/53 [00:16<00:00,  3.29it/s]\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:04<00:00,  9.40it/s]\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:05<00:00,  8.89it/s]\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:05<00:00,  6.83it/s]\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:03<00:00,  9.20it/s]\n",
      "Generating embeddings: 100%|██████████| 90/90 [00:16<00:00,  5.45it/s]\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:05<00:00,  6.95it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:02<00:00,  4.65it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:03<00:00,  6.61it/s]\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:07<00:00,  4.02it/s]\n",
      "Generating embeddings: 100%|██████████| 35/35 [00:04<00:00,  7.98it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:04<00:00,  5.21it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:02<00:00,  6.34it/s]\n",
      "Generating embeddings: 100%|██████████| 78/78 [00:18<00:00,  4.17it/s]\n",
      "Generating embeddings: 100%|██████████| 25/25 [00:04<00:00,  5.39it/s]\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:05<00:00,  5.76it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:03<00:00,  7.16it/s]\n",
      "Generating embeddings: 100%|██████████| 80/80 [00:10<00:00,  7.51it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:03<00:00,  4.48it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:03<00:00,  6.52it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:03<00:00,  6.57it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:03<00:00,  8.61it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:03<00:00,  4.69it/s]\n",
      "Generating embeddings: 100%|██████████| 85/85 [00:19<00:00,  4.39it/s]\n",
      "Generating embeddings: 100%|██████████| 59/59 [00:18<00:00,  3.13it/s]\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:09<00:00,  4.16it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:06<00:00,  3.79it/s]\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:17<00:00,  2.04it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:01<00:00,  7.33it/s]\n",
      "Generating embeddings: 100%|██████████| 29/29 [00:04<00:00,  7.19it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:00<00:00, 10.85it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:03<00:00,  6.04it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:04<00:00,  1.95it/s]\n",
      "Generating embeddings: 100%|██████████| 74/74 [00:19<00:00,  3.85it/s]\n",
      "Generating embeddings: 100%|██████████| 55/55 [00:19<00:00,  2.83it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:00<00:00,  6.11it/s]\n",
      "Generating embeddings: 100%|██████████| 268/268 [01:34<00:00,  2.85it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:04<00:00,  4.72it/s]\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:06<00:00,  6.45it/s]\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:05<00:00,  6.52it/s]\n",
      "Generating embeddings: 100%|██████████| 52/52 [00:14<00:00,  3.60it/s]\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:16<00:00,  3.76it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  6.28it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  6.27it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:05<00:00,  3.34it/s]\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:12<00:00,  3.20it/s]\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:11<00:00,  3.02it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:04<00:00,  3.41it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.54it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.31it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.40it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.88it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.91it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.06it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:03<00:00,  4.04it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  8.13it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  7.92it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.36it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:02<00:00,  4.85it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:03<00:00,  3.46it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  5.62it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  5.69it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00,  8.38it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:02<00:00,  5.64it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:01<00:00,  5.01it/s]\n",
      "Generating embeddings: 100%|██████████| 116/116 [00:13<00:00,  8.47it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  7.27it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.87it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  8.93it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:03<00:00,  2.08it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  7.45it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  5.86it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  7.44it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  3.32it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  5.23it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  4.09it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  8.44it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00,  7.79it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:01<00:00,  7.81it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:05<00:00,  3.03it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  7.32it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 10.02it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.88it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.77it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  4.29it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:06<00:00,  3.30it/s]\n",
      "Generating embeddings: 100%|██████████| 22/22 [00:07<00:00,  3.01it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.47it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:00<00:00,  7.13it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:01<00:00,  5.89it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  6.74it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.97it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.45it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  7.60it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:05<00:00,  3.02it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.50it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.47it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.06it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  8.59it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:02<00:00,  5.62it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.52it/s]\n",
      "Generating embeddings: 100%|██████████| 18/18 [00:07<00:00,  2.53it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.01it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.78it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.81it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 13.16it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:01<00:00,  3.17it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.48it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:07<00:00,  3.42it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.56it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.14it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.97it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.92it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.69it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.13it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.90it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.72it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00,  8.03it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.73it/s]\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:13<00:00,  2.74it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:01<00:00,  2.01it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:02<00:00,  2.40it/s]\n",
      "Generating embeddings: 100%|██████████| 30/30 [00:10<00:00,  2.92it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.48it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.74it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  7.46it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:03<00:00,  4.26it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.54it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.46it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  6.26it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.93it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.16it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.66it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  4.50it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.29it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:02<00:00,  5.98it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.03it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]\n",
      "Generating embeddings: 100%|██████████| 51/51 [00:15<00:00,  3.30it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:00<00:00,  7.88it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:07<00:00,  1.85it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  5.34it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.08it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.71it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.01it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.73it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 10.83it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 11.85it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.34it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  8.20it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 11.82it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:02<00:00,  2.90it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  9.83it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  7.51it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  4.78it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  8.49it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.69it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:01<00:00,  3.42it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 10.81it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00, 12.28it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:01<00:00,  5.09it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:02<00:00,  4.10it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  7.37it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  4.26it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:02<00:00,  8.97it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:01<00:00,  5.74it/s]\n",
      "Generating embeddings: 100%|██████████| 21/21 [00:03<00:00,  6.44it/s]\n",
      "Generating embeddings: 100%|██████████| 96/96 [00:24<00:00,  3.99it/s]\n",
      "Generating embeddings: 100%|██████████| 81/81 [00:19<00:00,  4.25it/s]\n",
      "Generating embeddings: 100%|██████████| 141/141 [00:30<00:00,  4.64it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:03<00:00,  4.26it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:03<00:00,  2.80it/s]\n",
      "Generating embeddings: 100%|██████████| 40/40 [00:08<00:00,  4.59it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:06<00:00,  4.10it/s]\n",
      "Generating embeddings: 100%|██████████| 16/16 [00:06<00:00,  2.32it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:07<00:00,  3.59it/s]\n",
      "Generating embeddings: 100%|██████████| 108/108 [00:21<00:00,  4.93it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:01<00:00,  5.38it/s]\n",
      "Generating embeddings: 100%|██████████| 76/76 [00:11<00:00,  6.86it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:01<00:00, 10.53it/s]\n",
      "Generating embeddings: 100%|██████████| 339/339 [01:08<00:00,  4.92it/s]\n",
      "Generating embeddings: 100%|██████████| 62/62 [00:20<00:00,  3.00it/s]\n",
      "Generating embeddings: 100%|██████████| 80/80 [00:09<00:00,  8.49it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:01<00:00,  6.34it/s]\n",
      "Generating embeddings: 100%|██████████| 57/57 [00:17<00:00,  3.27it/s]\n",
      "Generating embeddings: 100%|██████████| 138/138 [00:20<00:00,  6.69it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:01<00:00,  5.04it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:02<00:00,  6.37it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:03<00:00,  5.14it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:03<00:00,  7.93it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  6.00it/s]\n",
      "Generating embeddings: 100%|██████████| 92/92 [00:20<00:00,  4.48it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:05<00:00,  4.78it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:01<00:00,  8.52it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:01<00:00,  8.48it/s]\n",
      "Generating embeddings: 100%|██████████| 73/73 [00:19<00:00,  3.70it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  6.24it/s]\n",
      "Generating embeddings: 100%|██████████| 9/9 [00:01<00:00,  6.69it/s]\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:09<00:00,  4.90it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  6.85it/s]\n",
      "Generating embeddings: 100%|██████████| 77/77 [00:09<00:00,  8.48it/s]\n",
      "Generating embeddings: 100%|██████████| 50/50 [00:12<00:00,  4.10it/s]\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:06<00:00,  6.24it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:05<00:00,  4.59it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  6.36it/s]\n",
      "Generating embeddings: 100%|██████████| 23/23 [00:11<00:00,  2.04it/s]\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:04<00:00, 11.59it/s]\n",
      "Generating embeddings: 100%|██████████| 87/87 [00:19<00:00,  4.41it/s]\n",
      "Generating embeddings: 100%|██████████| 119/119 [00:18<00:00,  6.54it/s]\n",
      "Generating embeddings: 100%|██████████| 97/97 [00:22<00:00,  4.30it/s]\n",
      "Generating embeddings: 100%|██████████| 121/121 [00:27<00:00,  4.39it/s]\n",
      "Generating embeddings: 100%|██████████| 294/294 [01:15<00:00,  3.87it/s]\n",
      "Generating embeddings: 100%|██████████| 407/407 [01:43<00:00,  3.95it/s]\n",
      "Generating embeddings: 100%|██████████| 42/42 [00:11<00:00,  3.70it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:00<00:00,  7.26it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:01<00:00,  2.01it/s]\n",
      "Generating embeddings: 100%|██████████| 31/31 [00:07<00:00,  4.03it/s]\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:10<00:00,  3.76it/s]\n",
      "Generating embeddings: 100%|██████████| 37/37 [00:09<00:00,  3.79it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:05<00:00,  5.27it/s]\n",
      "Generating embeddings: 100%|██████████| 54/54 [00:12<00:00,  4.47it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:05<00:00,  5.05it/s]\n",
      "Generating embeddings: 100%|██████████| 46/46 [00:05<00:00,  8.26it/s]\n",
      "Generating embeddings: 100%|██████████| 26/26 [00:05<00:00,  4.73it/s]\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:09<00:00,  3.73it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:05<00:00,  5.18it/s]\n",
      "Generating embeddings: 100%|██████████| 94/94 [00:16<00:00,  5.57it/s]\n",
      "Generating embeddings: 100%|██████████| 55/55 [00:11<00:00,  4.63it/s]\n",
      "Generating embeddings: 100%|██████████| 36/36 [00:08<00:00,  4.26it/s]\n",
      "Generating embeddings: 100%|██████████| 39/39 [00:09<00:00,  4.23it/s]\n",
      "Generating embeddings: 100%|██████████| 31/31 [00:05<00:00,  5.38it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 13.85it/s]\n",
      "Generating embeddings: 100%|██████████| 86/86 [00:14<00:00,  5.78it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:03<00:00,  2.86it/s]\n",
      "Generating embeddings: 100%|██████████| 117/117 [00:21<00:00,  5.36it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:01<00:00,  3.79it/s]\n",
      "Generating embeddings: 100%|██████████| 81/81 [00:13<00:00,  6.04it/s]\n",
      "Generating embeddings: 100%|██████████| 93/93 [00:15<00:00,  5.88it/s]\n",
      "Generating embeddings: 100%|██████████| 121/121 [00:31<00:00,  3.86it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  6.15it/s]\n",
      "Generating embeddings: 100%|██████████| 124/124 [00:31<00:00,  3.96it/s]\n",
      "Generating embeddings: 100%|██████████| 54/54 [00:08<00:00,  6.19it/s]\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  7.94it/s]\n",
      "Generating embeddings: 100%|██████████| 58/58 [00:15<00:00,  3.77it/s]\n",
      "Generating embeddings: 100%|██████████| 114/114 [00:20<00:00,  5.43it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00, 14.00it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:03<00:00,  2.62it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00,  8.21it/s]\n",
      "Generating embeddings: 100%|██████████| 55/55 [00:07<00:00,  7.02it/s]\n",
      "Generating embeddings: 100%|██████████| 85/85 [00:09<00:00,  8.55it/s]\n",
      "Generating embeddings: 100%|██████████| 94/94 [00:20<00:00,  4.65it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:01<00:00, 15.82it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  5.28it/s]\n",
      "Generating embeddings: 100%|██████████| 81/81 [00:30<00:00,  2.65it/s]\n",
      "Generating embeddings: 100%|██████████| 142/142 [00:22<00:00,  6.23it/s]\n",
      "Generating embeddings: 100%|██████████| 15/15 [00:02<00:00,  5.89it/s]\n",
      "Generating embeddings: 100%|██████████| 80/80 [00:07<00:00, 10.74it/s]\n",
      "Generating embeddings: 100%|██████████| 94/94 [00:14<00:00,  6.67it/s]\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:04<00:00, 10.24it/s]\n",
      "Generating embeddings: 100%|██████████| 118/118 [00:11<00:00,  9.91it/s]\n",
      "Generating embeddings: 100%|██████████| 35/35 [00:04<00:00,  8.29it/s]\n",
      "Generating embeddings: 100%|██████████| 17/17 [00:02<00:00,  6.90it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:01<00:00,  4.15it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  4.71it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:03<00:00,  2.18it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00, 10.25it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:00<00:00,  8.90it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  8.68it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:01<00:00,  4.05it/s]\n",
      "Generating embeddings: 100%|██████████| 40/40 [00:09<00:00,  4.35it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:02<00:00,  4.13it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00, 11.06it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:00<00:00,  6.08it/s]\n",
      "Generating embeddings: 100%|██████████| 14/14 [00:01<00:00,  7.95it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  5.99it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:01<00:00,  8.79it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:04<00:00,  2.67it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:01<00:00,  4.80it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:04<00:00,  2.74it/s]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:00<00:00,  5.23it/s]\n",
      "Generating embeddings: 100%|██████████| 6/6 [00:01<00:00,  5.90it/s]\n",
      "Generating embeddings: 100%|██████████| 19/19 [00:02<00:00,  6.45it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:02<00:00,  4.43it/s]\n",
      "Generating embeddings: 100%|██████████| 41/41 [00:08<00:00,  4.73it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:01<00:00,  5.50it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:01<00:00,  6.14it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:00<00:00,  8.64it/s]\n",
      "Generating embeddings: 100%|██████████| 20/20 [00:01<00:00, 10.65it/s]\n",
      "Generating embeddings: 100%|██████████| 12/12 [00:01<00:00,  9.44it/s]\n",
      "Generating embeddings: 100%|██████████| 13/13 [00:02<00:00,  6.41it/s]\n",
      "Generating embeddings: 100%|██████████| 5/5 [00:02<00:00,  2.21it/s]\n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00,  7.18it/s]\n",
      "Generating embeddings: 100%|██████████| 28/28 [00:04<00:00,  6.12it/s]\n",
      "Generating embeddings: 100%|██████████| 8/8 [00:02<00:00,  2.84it/s]\n",
      "Generating embeddings: 100%|██████████| 24/24 [00:10<00:00,  2.38it/s]\n",
      "Parsing nodes: 100%|██████████| 333/333 [31:51<00:00,  5.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "cortex_search_pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        splitter,\n",
    "    ],\n",
    ")\n",
    "\n",
    "results = cortex_search_pipeline.run(show_progress=True, documents=cleaned_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1698/1698 [16:56<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "snowflake_connector = snowflake.connector.connect(**connection_params)\n",
    "\n",
    "cursor = snowflake_connector.cursor()\n",
    "\n",
    "cursor.execute(\"CREATE OR REPLACE TABLE streamlit_docs(doc_text VARCHAR)\")\n",
    "for curr in tqdm(results):\n",
    "    cursor.execute(\"INSERT INTO streamlit_docs VALUES (%s)\", curr.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.core import Root\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CortexSearchRetriever:\n",
    "\n",
    "    def __init__(self, snowpark_session: Session, limit_to_retrieve: int = 4):\n",
    "        self._snowpark_session = snowpark_session\n",
    "        self._limit_to_retrieve = limit_to_retrieve\n",
    "\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        root = Root(self._snowpark_session)\n",
    "        cortex_search_service = (\n",
    "            root.databases[os.getenv(\"SNOWFLAKE_DATABASE\")]\n",
    "            .schemas[os.getenv(\"SNOWFLAKE_SCHEMA\")]\n",
    "            .cortex_search_services[os.getenv(\"SNOWFLAKE_CORTEX_SEARCH_SERVICE\")]\n",
    "        )\n",
    "        resp = cortex_search_service.search(\n",
    "            query=query,\n",
    "            columns=[\"doc_text\"],\n",
    "            limit=self._limit_to_retrieve,\n",
    "        )\n",
    "\n",
    "        if resp.results:\n",
    "            return [curr[\"doc_text\"] for curr in resp.results]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CortexSearchRetriever(snowpark_session=snowpark_session, limit_to_retrieve=4)\n",
    "\n",
    "retrieved_context = retriever.retrieve(query=\"How do I launch a streamlit app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<Note>\\n\\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the\\narguments get interpreted as arguments to Streamlit itself.\\n\\n</Note>\\n\\nAnother way of running Streamlit is to run it as a Python module. This can be\\nuseful when configuring an IDE like PyCharm to work with Streamlit:\\n\\n```bash\\n# Running\\npython -m streamlit run your_script.py\\n\\n# is equivalent to:\\nstreamlit run your_script.py\\n```\\n\\n<Tip>\\n\\nYou can also pass a URL to `streamlit run`! This is great when combined with\\nGitHub Gists. For example:\\n\\n```bash\\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n```\\n\\n</Tip>\\n\\n## Development flow\\n\\nEvery time you want to update your app, save the source file. When you do\\nthat, Streamlit detects if there is a change and asks you whether you want to\\nrerun your app. Choose \"Always rerun\" at the top-right of your screen to\\nautomatically update your app every time you change its source code.\\n\\nThis allows you to work in a fast interactive loop: you type some code, save\\nit, try it out live, then type some more code, save it, try it out, and so on\\nuntil you\\'re happy with the results. This tight loop between coding and viewing\\nresults live is one of the ways Streamlit makes your life easier.\\n\\n<Tip>\\n\\nWhile developing a Streamlit app, it\\'s recommended to lay out your editor and\\nbrowser windows side by side, so the code and the app can be seen at the same\\ntime. Give it a try!\\n\\n</Tip>\\n\\nAs of Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. If you try to run a Streamlit app from the root directory, Streamlit will throw a `FileNotFoundError: [Errno 2] No such file or directory` error. For more information, see GitHub issue [#5239](https://github.com/streamlit/streamlit/issues/5239).\\n\\nIf you are using Streamlit version 1.10.0 or higher, your main script should live in a directory other than the root directory. When using Docker, you can use the `WORKDIR` command to specify the directory where your main script lives. For an example of how to do this, read [Create a Dockerfile](/deploy/tutorials/docker#create-a-dockerfile).\\n\\n## Data flow\\n\\nStreamlit\\'s architecture allows you to write apps the same way you write plain\\nPython scripts. To unlock this, Streamlit apps have a unique data flow: any\\ntime something must be updated on the screen, Streamlit reruns your entire\\nPython script from top to bottom.\\n\\nThis can happen in two situations:\\n\\n- Whenever you modify your app\\'s source code.\\n\\n',\n",
       " \"We love to hear your questions, ideas, and help you work through your bugs — stop by today!\\n\\n1. The first step is to create a new Python script. Let's call it\\n   `uber_pickups.py`.\\n\\n2. Open `uber_pickups.py` in your favorite IDE or text editor, then add these\\n   lines:\\n\\n   ```python\\n   import streamlit as st\\n   import pandas as pd\\n   import numpy as np\\n   ```\\n\\n3. Every good app has a title, so let's add one:\\n\\n   ```python\\n   st.title('Uber pickups in NYC')\\n   ```\\n\\n4. Now it's time to run Streamlit from the command line:\\n\\n   ```bash\\n   streamlit run uber_pickups.py\\n   ```\\n\\n   Running a Streamlit app is no different than any other Python script. Whenever you need to view the app, you can use this command.\\n\\n   <Tip>\\n\\n   Did you know you can also pass a URL to `streamlit run`? This is great when combined with GitHub Gists. For example:\\n\\n   ```bash\\n   streamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n   ```\\n\\n   </Tip>\\n\\n5. As usual, the app should automatically open in a new tab in your\\n   browser.\\n\\n## Fetch some data\\n\\nNow that you have an app, the next thing you'll need to do is fetch the Uber\\ndataset for pickups and drop-offs in New York City.\\n\\n1. \",\n",
       " '---\\ntitle: Run your Streamlit app\\nslug: /develop/concepts/architecture/run-your-app\\n---\\n\\n# Run your Streamlit app\\n\\nWorking with Streamlit is simple. First you sprinkle a few Streamlit commands into a normal Python script, and then you run it. ',\n",
       " \"First, where do you want to run your Streamlit app, and how do you want to access it?\\n\\n- **On your corporate network**\\xa0- Most corporate networks are closed to the outside world. You typically use a VPN to log onto your corporate network and access resources there. You could run your Streamlit app on a server in your corporate network for security reasons, to ensure that only folks internal to your company can access it.\\n- **On the cloud**\\xa0- If you'd like to access your Streamlit app from outside of a corporate network, or share your app with folks outside of your home network or laptop, you might choose this option. In this case, it'll depend on your hosting provider. We have [community-submitted guides](/knowledge-base/deploy/deploy-streamlit-heroku-aws-google-cloud) from Heroku, AWS, and other providers.\\n\\nWherever you decide to deploy your app, you will first need to containerize it. This guide walks you through using Docker to deploy your app. If you prefer Kubernetes see [Deploy Streamlit using Kubernetes](/deploy/tutorials/kubernetes).\\n\\n## Prerequisites\\n\\n1. \"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running the TruLens dashboard requires providing a `password` to the `SnowflakeConnector`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Initialized with db url snowflake://JUNWEI:***@uxccqxi-hijw/LLMOPS_DB/LLMOPS_SCHEMA?role=ACCOUNTADMIN&warehouse=LLMOPS_WH_M .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error setting TruLens workspace version tag: 000002 (0A000): Unsupported feature 'TAG'., check if you have enterprise version of Snowflake.\n"
     ]
    }
   ],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.connectors.snowflake import SnowflakeConnector\n",
    "\n",
    "tru_snowflake_connector = SnowflakeConnector(snowpark_session=snowpark_session)\n",
    "\n",
    "tru_session = TruSession(connector=tru_snowflake_connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decorating <function RAG_from_scratch.retrieve_context at 0x0000020389184B80>\n",
      "decorating <function RAG_from_scratch.generate_completion at 0x0000020389184C10>\n",
      "decorating <function RAG_from_scratch.query at 0x0000020389184790>\n",
      "adding method <class '__main__.RAG_from_scratch'> retrieve_context __main__\n",
      "adding method <class '__main__.RAG_from_scratch'> generate_completion __main__\n",
      "adding method <class '__main__.RAG_from_scratch'> query __main__\n"
     ]
    }
   ],
   "source": [
    "from trulens.apps.custom import instrument\n",
    "from snowflake.cortex import Complete\n",
    "\n",
    "\n",
    "class RAG_from_scratch:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.retriever = CortexSearchRetriever(snowpark_session=snowpark_session, limit_to_retrieve=4)\n",
    "\n",
    "    @instrument\n",
    "    def retrieve_context(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        return self.retriever.retrieve(query)\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "          You are an expert assistant extracting information from context provided.\n",
    "          Answer the question based on the context. Be concise and do not hallucinate.\n",
    "          If you don´t have the information just say so.\n",
    "          Context: {context_str}\n",
    "          Question:\n",
    "          {query}\n",
    "          Answer:\n",
    "        \"\"\"\n",
    "        return Complete(\"mistral-large\", prompt)\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve_context(query)\n",
    "        return self.generate_completion(query, context_str)\n",
    "\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You can launch a Streamlit app by running it as a Python module with the command `python -m streamlit run your_script.py` or directly with `streamlit run your_script.py`. You can also pass a URL to `streamlit run` to launch an app from a remote location, such as a GitHub Gist.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.query(\"How do I launch a streamlit app?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve_context.rets[:].collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input context will be set to __record__.app.retrieve_context.rets[:] .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens.providers.cortex.provider import Cortex\n",
    "from trulens.core import Feedback\n",
    "from trulens.core import Select\n",
    "import numpy as np\n",
    "\n",
    "provider = Cortex(snowpark_session, \"llama3.1-8b\")\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:].collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance, name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:])\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance, name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class '__main__.RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_completion\n",
      "\tinstrumenting query\n",
      "skipping base <class 'object'> because of class\n",
      "skipping base <class '__main__.CortexSearchRetriever'> because of class\n",
      "skipping base <class 'object'> because of class\n"
     ]
    }
   ],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_rag = TruCustomApp(\n",
    "    rag,\n",
    "    app_name=\"RAG\",\n",
    "    app_version=\"simple\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"How do I launch a streamlit app?\",\n",
    "    \"How can I capture the state of my session in streamlit?\",\n",
    "    # \"How do I install streamlit?\",\n",
    "    # \"How do I change the background color of a streamlit app?\",\n",
    "    # \"What's the advantage of using a streamlit form?\",\n",
    "    # \"What are some ways I should use checkboxes?\",\n",
    "    # \"How can I conserve space and hide away content?\",\n",
    "    # \"Can you recommend some resources for learning Streamlit?\",\n",
    "    # \"What are some common use cases for Streamlit?\",\n",
    "    # \"How can I deploy a streamlit app on the cloud?\",\n",
    "    # \"How do I add a logo to streamlit?\",\n",
    "    # \"What is the best way to deploy a Streamlit app?\",\n",
    "    # \"How should I use a streamlit toggle?\",\n",
    "    # \"How do I add new pages to my streamlit app?\",\n",
    "    # \"How do I write a dataframe to display in my dashboard?\",\n",
    "    # \"Can I plot a map in streamlit? If so, how?\",\n",
    "    # \"How do vector stores enable efficient similarity search?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling <function RAG_from_scratch.query at 0x0000020389184790> with (<__main__.RAG_from_scratch object at 0x000002037EB92560>, 'How do I launch a streamlit app?')\n",
      "calling <function RAG_from_scratch.retrieve_context at 0x0000020389184B80> with (<__main__.RAG_from_scratch object at 0x000002037EB92560>, 'How do I launch a streamlit app?')\n",
      "calling <function RAG_from_scratch.generate_completion at 0x0000020389184C10> with (<__main__.RAG_from_scratch object at 0x000002037EB92560>, 'How do I launch a streamlit app?', ['<Note>\\n\\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the\\narguments get interpreted as arguments to Streamlit itself.\\n\\n</Note>\\n\\nAnother way of running Streamlit is to run it as a Python module. This can be\\nuseful when configuring an IDE like PyCharm to work with Streamlit:\\n\\n```bash\\n# Running\\npython -m streamlit run your_script.py\\n\\n# is equivalent to:\\nstreamlit run your_script.py\\n```\\n\\n<Tip>\\n\\nYou can also pass a URL to `streamlit run`! This is great when combined with\\nGitHub Gists. For example:\\n\\n```bash\\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n```\\n\\n</Tip>\\n\\n## Development flow\\n\\nEvery time you want to update your app, save the source file. When you do\\nthat, Streamlit detects if there is a change and asks you whether you want to\\nrerun your app. Choose \"Always rerun\" at the top-right of your screen to\\nautomatically update your app every time you change its source code.\\n\\nThis allows you to work in a fast interactive loop: you type some code, save\\nit, try it out live, then type some more code, save it, try it out, and so on\\nuntil you\\'re happy with the results. This tight loop between coding and viewing\\nresults live is one of the ways Streamlit makes your life easier.\\n\\n<Tip>\\n\\nWhile developing a Streamlit app, it\\'s recommended to lay out your editor and\\nbrowser windows side by side, so the code and the app can be seen at the same\\ntime. Give it a try!\\n\\n</Tip>\\n\\nAs of Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. If you try to run a Streamlit app from the root directory, Streamlit will throw a `FileNotFoundError: [Errno 2] No such file or directory` error. For more information, see GitHub issue [#5239](https://github.com/streamlit/streamlit/issues/5239).\\n\\nIf you are using Streamlit version 1.10.0 or higher, your main script should live in a directory other than the root directory. When using Docker, you can use the `WORKDIR` command to specify the directory where your main script lives. For an example of how to do this, read [Create a Dockerfile](/deploy/tutorials/docker#create-a-dockerfile).\\n\\n## Data flow\\n\\nStreamlit\\'s architecture allows you to write apps the same way you write plain\\nPython scripts. To unlock this, Streamlit apps have a unique data flow: any\\ntime something must be updated on the screen, Streamlit reruns your entire\\nPython script from top to bottom.\\n\\nThis can happen in two situations:\\n\\n- Whenever you modify your app\\'s source code.\\n\\n', \"We love to hear your questions, ideas, and help you work through your bugs — stop by today!\\n\\n1. The first step is to create a new Python script. Let's call it\\n   `uber_pickups.py`.\\n\\n2. Open `uber_pickups.py` in your favorite IDE or text editor, then add these\\n   lines:\\n\\n   ```python\\n   import streamlit as st\\n   import pandas as pd\\n   import numpy as np\\n   ```\\n\\n3. Every good app has a title, so let's add one:\\n\\n   ```python\\n   st.title('Uber pickups in NYC')\\n   ```\\n\\n4. Now it's time to run Streamlit from the command line:\\n\\n   ```bash\\n   streamlit run uber_pickups.py\\n   ```\\n\\n   Running a Streamlit app is no different than any other Python script. Whenever you need to view the app, you can use this command.\\n\\n   <Tip>\\n\\n   Did you know you can also pass a URL to `streamlit run`? This is great when combined with GitHub Gists. For example:\\n\\n   ```bash\\n   streamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n   ```\\n\\n   </Tip>\\n\\n5. As usual, the app should automatically open in a new tab in your\\n   browser.\\n\\n## Fetch some data\\n\\nNow that you have an app, the next thing you'll need to do is fetch the Uber\\ndataset for pickups and drop-offs in New York City.\\n\\n1. \", '---\\ntitle: Run your Streamlit app\\nslug: /develop/concepts/architecture/run-your-app\\n---\\n\\n# Run your Streamlit app\\n\\nWorking with Streamlit is simple. First you sprinkle a few Streamlit commands into a normal Python script, and then you run it. ', \"First, where do you want to run your Streamlit app, and how do you want to access it?\\n\\n- **On your corporate network**\\xa0- Most corporate networks are closed to the outside world. You typically use a VPN to log onto your corporate network and access resources there. You could run your Streamlit app on a server in your corporate network for security reasons, to ensure that only folks internal to your company can access it.\\n- **On the cloud**\\xa0- If you'd like to access your Streamlit app from outside of a corporate network, or share your app with folks outside of your home network or laptop, you might choose this option. In this case, it'll depend on your hosting provider. We have [community-submitted guides](/knowledge-base/deploy/deploy-streamlit-heroku-aws-google-cloud) from Heroku, AWS, and other providers.\\n\\nWherever you decide to deploy your app, you will first need to containerize it. This guide walks you through using Docker to deploy your app. If you prefer Kubernetes see [Deploy Streamlit using Kubernetes](/deploy/tutorials/kubernetes).\\n\\n## Prerequisites\\n\\n1. \"])\n",
      "calling <function RAG_from_scratch.query at 0x0000020389184790> with (<__main__.RAG_from_scratch object at 0x000002037EB92560>, 'How can I capture the state of my session in streamlit?')\n",
      "calling <function RAG_from_scratch.retrieve_context at 0x0000020389184B80> with (<__main__.RAG_from_scratch object at 0x000002037EB92560>, 'How can I capture the state of my session in streamlit?')\n",
      "calling <function RAG_from_scratch.generate_completion at 0x0000020389184C10> with (<__main__.RAG_from_scratch object at 0x000002037EB92560>, 'How can I capture the state of my session in streamlit?', ['For each browser tab that connects to the Streamlit server, a new session is created. Streamlit reruns your script from top to bottom every time you interact with your app. Each reruns takes place in a blank slate: no variables are shared between runs.\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across pages inside a [multipage app](/develop/concepts/multipage-apps).\\n\\n', 'Streamlit makes this easy through the use of [Session State](/develop/concepts/architecture/session-state). If a `key` parameter is set, Streamlit will store any changes made to the dataframe in Session State.\\n\\nThis snippet shows how you can access changed data using Session State:\\n\\n```python\\nst.data_editor(df, key=\"my_key\", num_rows=\"dynamic\") # 👈 Set a key\\nst.write(\"Here\\'s the value in Session State:\")\\nst.write(st.session_state[\"my_key\"]) # 👈 Show the value in Session State\\n```\\n\\nIn this code snippet, the `key` parameter is set to `\"my_key\"`. After the data editor is created, the value associated to `\"my_key\"` in Session State is displayed in the app using `st.write`. This shows the additions, edits, and deletions that were made.\\n\\n', '---\\ntitle: Session State\\nslug: /develop/api-reference/caching-and-state/st.session_state\\ndescription: st.session_state is a way to share variables between reruns, for each user session.\\n---\\n\\n# Session State\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across apps inside a [multipage app](/develop/concepts/multipage-apps).\\n\\nCheck out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\\n\\n<YouTube videoId=\"92jUAXBmZyU\" />\\n\\n### Initialize values in Session State\\n\\nThe Session State API follows a field-based API, which is very similar to Python dictionaries:\\n\\n```python\\n# Initialization\\nif \\'key\\' not in st.session_state:\\n    st.session_state[\\'key\\'] = \\'value\\'\\n\\n# Session State also supports attribute based syntax\\nif \\'key\\' not in st.session_state:\\n    st.session_state.key = \\'value\\'\\n```\\n\\n### Reads and updates\\n\\nRead the value of an item in Session State and display it by passing to `st.write` :\\n\\n```python\\n# Read\\nst.write(st.session_state.key)\\n\\n# Outputs: value\\n```\\n\\nUpdate an item in Session State by assigning it a value:\\n\\n```python\\nst.session_state.key = \\'value2\\'     # Attribute API\\nst.session_state[\\'key\\'] = \\'value2\\'  # Dictionary like API\\n```\\n\\nCurious about what is in Session State? Use `st.write` or magic:\\n\\n```python\\nst.write(st.session_state)\\n\\n# With magic:\\nst.session_state\\n```\\n\\nStreamlit throws a handy exception if an uninitialized variable is accessed:\\n\\n```python\\nst.write(st.session_state[\\'value\\'])\\n\\n# Throws an exception!\\n', 'slug: /knowledge-base/using-streamlit/serializable-session-state\\n---\\n\\n# What is serializable session state?\\n\\n## Serializable Session State\\n\\nSerialization refers to the process of converting an object or data structure into a format that can be persisted and shared, and allowing you to recover the data’s original structure. Python’s built-in [pickle](https://docs.python.org/3/library/pickle.html) module serializes Python objects to a byte stream (\"pickling\") and deserializes the stream into an object (\"unpickling\").\\n\\nBy default, Streamlit’s [Session State](/develop/concepts/architecture/session-state) allows you to persist any Python object for the duration of the session, irrespective of the object’s pickle-serializability. This property lets you store Python primitives such as integers, floating-point numbers, complex numbers and booleans, dataframes, and even [lambdas](https://docs.python.org/3/reference/expressions.html#lambda) returned by functions. However, some execution environments may require serializing all data in Session State, so it may be useful to detect incompatibility during development, or when the execution environment will stop supporting it in the future.\\n\\n'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RAG</th>\n",
       "      <th>simple</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.651037</td>\n",
       "      <td>0.684355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Answer Relevance  Context Relevance  Groundedness  \\\n",
       "app_name app_version                                                      \n",
       "RAG      simple                    1.0           0.666667           1.0   \n",
       "\n",
       "                       latency  total_cost  \n",
       "app_name app_version                        \n",
       "RAG      simple       4.651037    0.684355  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tru_rag as recording:\n",
    "    for prompt in prompts:\n",
    "        rag.query(prompt)\n",
    "\n",
    "tru_session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decorating <function context_filter.__call__.<locals>.wrapper at 0x000002038CF6DF30>\n",
      "adding method <class '__main__.filtered_RAG_from_scratch'> retrieve_context __main__\n"
     ]
    }
   ],
   "source": [
    "from trulens.core.guardrails.base import context_filter\n",
    "\n",
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = Feedback(\n",
    "    provider.context_relevance, name=\"Context Relevance\"\n",
    ")\n",
    "\n",
    "\n",
    "class filtered_RAG_from_scratch(RAG_from_scratch):\n",
    "\n",
    "    @instrument\n",
    "    @context_filter(f_context_relevance_score, 0.75, keyword_for_prompt=\"query\")\n",
    "    def retrieve_context(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        return self.retriever.retrieve(query)\n",
    "\n",
    "\n",
    "filtered_rag = filtered_RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.filtered_RAG_from_scratch'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_completion\n",
      "\tinstrumenting query\n",
      "instrumenting <class '__main__.filtered_RAG_from_scratch'> for base <class '__main__.RAG_from_scratch'>\n",
      "\tinstrumenting retrieve_context\n",
      "\tinstrumenting generate_completion\n",
      "\tinstrumenting query\n",
      "skipping base <class 'object'> because of class\n",
      "skipping base <class '__main__.CortexSearchRetriever'> because of class\n",
      "skipping base <class 'object'> because of class\n"
     ]
    }
   ],
   "source": [
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_filtered_rag = TruCustomApp(\n",
    "    filtered_rag,\n",
    "    app_name=\"RAG\",\n",
    "    app_version=\"filtered\",\n",
    "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling <function RAG_from_scratch.query at 0x0000020389184790> with (<__main__.filtered_RAG_from_scratch object at 0x000002038CEA1300>, 'How do I launch a streamlit app?')\n",
      "calling <function context_filter.__call__.<locals>.wrapper at 0x000002038CF6DF30> with (<__main__.filtered_RAG_from_scratch object at 0x000002038CEA1300>, 'How do I launch a streamlit app?')\n",
      "calling <function RAG_from_scratch.generate_completion at 0x0000020389184C10> with (<__main__.filtered_RAG_from_scratch object at 0x000002038CEA1300>, 'How do I launch a streamlit app?', [])\n",
      "calling <function RAG_from_scratch.query at 0x0000020389184790> with (<__main__.filtered_RAG_from_scratch object at 0x000002038CEA1300>, 'How can I capture the state of my session in streamlit?')\n",
      "calling <function context_filter.__call__.<locals>.wrapper at 0x000002038CF6DF30> with (<__main__.filtered_RAG_from_scratch object at 0x000002038CEA1300>, 'How can I capture the state of my session in streamlit?')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\Desktop\\ragnroll_test\\getting_started_llmops\\lib\\site-packages\\trulens\\core\\feedback\\feedback.py:974: UserWarning: Feedback function Context Relevance with aggregation <function mean at 0x00000203686C13F0> had no inputs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling <function RAG_from_scratch.generate_completion at 0x0000020389184C10> with (<__main__.filtered_RAG_from_scratch object at 0x000002038CEA1300>, 'How can I capture the state of my session in streamlit?', ['For each browser tab that connects to the Streamlit server, a new session is created. Streamlit reruns your script from top to bottom every time you interact with your app. Each reruns takes place in a blank slate: no variables are shared between runs.\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across pages inside a [multipage app](/develop/concepts/multipage-apps).\\n\\n', 'Streamlit makes this easy through the use of [Session State](/develop/concepts/architecture/session-state). If a `key` parameter is set, Streamlit will store any changes made to the dataframe in Session State.\\n\\nThis snippet shows how you can access changed data using Session State:\\n\\n```python\\nst.data_editor(df, key=\"my_key\", num_rows=\"dynamic\") # 👈 Set a key\\nst.write(\"Here\\'s the value in Session State:\")\\nst.write(st.session_state[\"my_key\"]) # 👈 Show the value in Session State\\n```\\n\\nIn this code snippet, the `key` parameter is set to `\"my_key\"`. After the data editor is created, the value associated to `\"my_key\"` in Session State is displayed in the app using `st.write`. This shows the additions, edits, and deletions that were made.\\n\\n', '---\\ntitle: Session State\\nslug: /develop/api-reference/caching-and-state/st.session_state\\ndescription: st.session_state is a way to share variables between reruns, for each user session.\\n---\\n\\n# Session State\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across apps inside a [multipage app](/develop/concepts/multipage-apps).\\n\\nCheck out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\\n\\n<YouTube videoId=\"92jUAXBmZyU\" />\\n\\n### Initialize values in Session State\\n\\nThe Session State API follows a field-based API, which is very similar to Python dictionaries:\\n\\n```python\\n# Initialization\\nif \\'key\\' not in st.session_state:\\n    st.session_state[\\'key\\'] = \\'value\\'\\n\\n# Session State also supports attribute based syntax\\nif \\'key\\' not in st.session_state:\\n    st.session_state.key = \\'value\\'\\n```\\n\\n### Reads and updates\\n\\nRead the value of an item in Session State and display it by passing to `st.write` :\\n\\n```python\\n# Read\\nst.write(st.session_state.key)\\n\\n# Outputs: value\\n```\\n\\nUpdate an item in Session State by assigning it a value:\\n\\n```python\\nst.session_state.key = \\'value2\\'     # Attribute API\\nst.session_state[\\'key\\'] = \\'value2\\'  # Dictionary like API\\n```\\n\\nCurious about what is in Session State? Use `st.write` or magic:\\n\\n```python\\nst.write(st.session_state)\\n\\n# With magic:\\nst.session_state\\n```\\n\\nStreamlit throws a handy exception if an uninitialized variable is accessed:\\n\\n```python\\nst.write(st.session_state[\\'value\\'])\\n\\n# Throws an exception!\\n', 'slug: /knowledge-base/using-streamlit/serializable-session-state\\n---\\n\\n# What is serializable session state?\\n\\n## Serializable Session State\\n\\nSerialization refers to the process of converting an object or data structure into a format that can be persisted and shared, and allowing you to recover the data’s original structure. Python’s built-in [pickle](https://docs.python.org/3/library/pickle.html) module serializes Python objects to a byte stream (\"pickling\") and deserializes the stream into an object (\"unpickling\").\\n\\nBy default, Streamlit’s [Session State](/develop/concepts/architecture/session-state) allows you to persist any Python object for the duration of the session, irrespective of the object’s pickle-serializability. This property lets you store Python primitives such as integers, floating-point numbers, complex numbers and booleans, dataframes, and even [lambdas](https://docs.python.org/3/reference/expressions.html#lambda) returned by functions. However, some execution environments may require serializing all data in Session State, so it may be useful to detect incompatibility during development, or when the execution environment will stop supporting it in the future.\\n\\n'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">RAG</th>\n",
       "      <th>filtered</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.295568</td>\n",
       "      <td>0.585047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.651037</td>\n",
       "      <td>0.684355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Answer Relevance  Context Relevance  Groundedness  \\\n",
       "app_name app_version                                                      \n",
       "RAG      filtered                  1.0               1.00           NaN   \n",
       "         simple                    1.0               0.75           1.0   \n",
       "\n",
       "                       latency  total_cost  \n",
       "app_name app_version                        \n",
       "RAG      filtered     5.295568    0.585047  \n",
       "         simple       4.651037    0.684355  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tru_filtered_rag as recording:\n",
    "    for prompt in prompts:\n",
    "        filtered_rag.query(prompt)\n",
    "\n",
    "tru_session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getting_started_llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
